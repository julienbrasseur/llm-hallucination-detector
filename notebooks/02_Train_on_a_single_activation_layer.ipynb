{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4a88c3-87bc-47a3-8b39-db6675ecb897",
   "metadata": {},
   "source": [
    "# **First experiment: training a XGBoost probe on the best performing activation layer**\n",
    "\n",
    "In this notebook, we train an XGBoost probe on layer 16, previously identified as the most expressive layer for hallucination detection.\n",
    "\n",
    "Before proceeding with training, we must extract activations for the training set. Since activations for the validation and test splits were already extracted during layer selection, only the training split remains to be processed.\n",
    "\n",
    "### 1. Installing required libraries\n",
    "\n",
    "Before doing so, let's first install the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4b45b6b-7e9a-486d-934d-9b58b5abaa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/julienbrasseur/llm-hallucination-detector.git\n",
      "  Cloning https://github.com/julienbrasseur/llm-hallucination-detector.git to /tmp/pip-req-build-x61_od1e\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/julienbrasseur/llm-hallucination-detector.git /tmp/pip-req-build-x61_od1e\n",
      "  Resolved https://github.com/julienbrasseur/llm-hallucination-detector.git to commit 77b721d351f3cb5b08d8447d199d6afe38970d26\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llmscan==0.1.0) (2.4.1+cu124)\n",
      "Collecting transformers>=4.36.0 (from llmscan==0.1.0)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting xgboost>=2.0.0 (from llmscan==0.1.0)\n",
      "  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn>=1.3.0 (from llmscan==0.1.0)\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from llmscan==0.1.0) (1.26.3)\n",
      "Collecting scipy>=1.11.0 (from llmscan==0.1.0)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting tqdm>=4.65.0 (from llmscan==0.1.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting accelerate>=0.25.0 (from llmscan==0.1.0)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting matplotlib>=3.7.0 (from llmscan==0.1.0)\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (6.0.2)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (2024.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (0.27.2)\n",
      "Collecting shellingham (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading typer_slim-0.20.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (0.14.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llmscan==0.1.0) (10.2.0)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llmscan==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llmscan==0.1.0) (1.16.0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn>=1.3.0->llmscan==0.1.0)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn>=1.3.0->llmscan==0.1.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.0.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.36.0->llmscan==0.1.0)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->llmscan==0.1.0) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.36.0->llmscan==0.1.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->llmscan==0.1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->llmscan==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->llmscan==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->llmscan==0.1.0) (1.3.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m134.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m153.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m131.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m154.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llmscan\n",
      "  Building wheel for llmscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llmscan: filename=llmscan-0.1.0-py3-none-any.whl size=18859 sha256=9cb82c4f12f3350c5c05db06d0350d676f046d0fe00e1f62e36c7b80f3dc41fc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-s_n89rwf/wheels/eb/e6/51/577f098f1ff2729ce5349f16e3327eba87b9c2b9d3ea4df270\n",
      "Successfully built llmscan\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, safetensors, regex, pyparsing, kiwisolver, joblib, hf-xet, fonttools, cycler, contourpy, xgboost, scikit-learn, matplotlib, huggingface_hub, tokenizers, transformers, accelerate, llmscan\n",
      "\u001b[2K  Attempting uninstall: pyparsing━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: pyparsing 2.4.7━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling pyparsing-2.4.7:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled pyparsing-2.4.7━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [llmscan]8/20\u001b[0m [accelerate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 hf-xet-1.2.0 huggingface_hub-0.36.0 joblib-1.5.3 kiwisolver-1.4.9 llmscan-0.1.0 matplotlib-3.10.8 pyparsing-3.2.5 regex-2025.11.3 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 xgboost-3.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m111.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]ess]lls]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 tzdata-2025.3 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install `llmscan`\n",
    "!pip install git+https://github.com/julienbrasseur/llm-hallucination-detector.git\n",
    "\n",
    "# Install `datasets`\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52e449a-283c-4efc-a18b-d79af939f2b4",
   "metadata": {},
   "source": [
    "### 2. Data preparation\n",
    "\n",
    "Now, we need to reload the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9db250-36cf-4c58-8b56-f7b1d6b47799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: krogoldAI/hallucination-labeled-dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c6310151a324b7898ccc14438d61662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02a502fa6b2240809a3d78e1d6e0f347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/78.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7801e61f7325492a9e14cb7466d80136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d70450a3f0f458b9881610472fb50a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dd91ffb6e9d4a48a983386f84f6691e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/101618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e2f8749a8c44754846e0cc2499d4d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21775 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25dbb939b6104a8b90e4f3f3ecc969e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/21776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and formatted:\n",
      "\tTrain:      101,618 examples\n",
      "\tValidation: 21,775 examples\n",
      "\tTest:       21,776 examples\n",
      "\tClass distribution (train): 68,913 non-hallucination, 32,705 hallucination\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set training dataset path\n",
    "DATASET_NAME = \"krogoldAI/hallucination-labeled-dataset\"\n",
    "\n",
    "def load_and_format_dataset(dataset_name: str):\n",
    "    \"\"\"\n",
    "    Load HuggingFace dataset and convert to conversation format.\n",
    "    \n",
    "    This function converts dataset with 'input', 'target', 'hallucination' fields\n",
    "    to the standard conversation format expected by the pipeline.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (train_data, val_data, test_data, train_labels, val_labels, test_labels)\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # Shuffle each split\n",
    "    ds[\"train\"] = ds[\"train\"].shuffle(seed=42)\n",
    "    ds[\"validation\"] = ds[\"validation\"].shuffle(seed=42)\n",
    "    ds[\"test\"] = ds[\"test\"].shuffle(seed=42)\n",
    "    \n",
    "    def format_split(split):\n",
    "        \"\"\"Convert HF dataset split to conversation format.\"\"\"\n",
    "        formatted = []\n",
    "        labels = []\n",
    "        \n",
    "        for item in split:\n",
    "            # Extract fields\n",
    "            user_msg = item[\"input\"]\n",
    "            assistant_msg = item[\"target\"]\n",
    "            label = int(item[\"hallucination\"])\n",
    "            \n",
    "            # Convert to OpenAI conversation format\n",
    "            formatted.append({\n",
    "                \"conversation\": [\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_msg},\n",
    "                ]\n",
    "            })\n",
    "            labels.append(label)\n",
    "        \n",
    "        return formatted, np.array(labels)\n",
    "    \n",
    "    # Format all splits\n",
    "    train_data, train_labels = format_split(ds[\"train\"])\n",
    "    val_data, val_labels = format_split(ds[\"validation\"])\n",
    "    test_data, test_labels = format_split(ds[\"test\"])\n",
    "    \n",
    "    print(f\"Dataset loaded and formatted:\")\n",
    "    print(f\"\\tTrain:      {len(train_data):,} examples\")\n",
    "    print(f\"\\tValidation: {len(val_data):,} examples\")\n",
    "    print(f\"\\tTest:       {len(test_data):,} examples\")\n",
    "    print(f\"\\tClass distribution (train): \"\n",
    "          f\"{(train_labels == 0).sum():,} non-hallucination, \"\n",
    "          f\"{(train_labels == 1).sum():,} hallucination\")\n",
    "    \n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels\n",
    "\n",
    "# Load and format dataset\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = \\\n",
    "    load_and_format_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128e76c0-08fa-4e94-9c65-0f93c9d19618",
   "metadata": {},
   "source": [
    "### 3. Extracting activations for the train split\n",
    "\n",
    "Now, as in the previous notebook, we will use the `ActivationExtractor` class to extract layer 16's activations for the train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59136867-f051-4034-ab59-9e9923411ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66416e6700a4f9caca779088708f49f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b4280225111483d8600c3392fe69ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5fc950e33f401292905000cbc7bcd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0818c290eb484ece9756973b7126453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e961e2db7b5e4425b7d98490f2ea264e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d602bb94b240e4b2f8b6093cfbda59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.07G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b74d382cbf54f31a441fea14de2fbd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ba3938967f4cf19d3cb91a2f869e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2604af5a21284f4fa4822972feda0a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "828c86807d594fcea135dd350a28aa9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model mistralai/Ministral-8B-Instruct-2410. Target layers: [16]. Device for inputs: cuda\n",
      "Extracting train activations...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting: 100%|██████████| 794/794 [1:04:49<00:00,  4.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 101618 train activation sequences\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from llmscan import ActivationExtractor\n",
    "\n",
    "# Initialize extractor\n",
    "extractor = ActivationExtractor(\n",
    "    model_name=\"mistralai/Ministral-8B-Instruct-2410\",\n",
    "    target_layers=[16], \n",
    "    device=\"cuda\"\n",
    ")\n",
    "\n",
    "# Extract test activations\n",
    "print(f\"Extracting train activations...\")\n",
    "activations = extractor.extract(\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    max_length=512,\n",
    "    mean_pool=True,\n",
    "    focus_on_assistant=True\n",
    ")\n",
    "\n",
    "# Save\n",
    "os.makedirs(\"feature_cache16\", exist_ok=True)\n",
    "torch.save(activations, f\"feature_cache16/train_activations_pooled.pt\")\n",
    "print(f\"Saved {len(activations)} train activation sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23165fbc-e61e-4150-bb71-3bc52e3e882a",
   "metadata": {},
   "source": [
    "*Remark:* Depending on the dataset size, this step can take some time, so a bit of patience is needed.\n",
    "\n",
    "*Remark:* Parameter `target_layer` in `ActivationExtractor` supports multi-layer extraction, so we could specify, e.g., `[15,16]` to simulaneously extract layers 15 and 16.\n",
    "\n",
    "*Remark:* Under the roof, the `extract` method of the `ActivationExtractor` class converts conversations from the standard OpenAI format to the model's native chat template. For instance:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I assist you today?\"}\n",
    "]\n",
    "```\n",
    "\n",
    "becomes:\n",
    "\n",
    "```\n",
    "<s>[INST]Hello![/INST]Hello! How can I assist you today?</s>\n",
    "```\n",
    "\n",
    "To isolate the assistant's response (on which we focus our analysis) the method automatically locates the final occurrence of the `[/INST]` token and extracts activations only from the tokens that follow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a152035-325d-4cd3-9831-1066fdcbc89e",
   "metadata": {},
   "source": [
    "### 4. Training a XGBoost probe on layer 16\n",
    "\n",
    "This being done, we can train a XGBoost probe on activation layer 16 using the `XGBoostProbe` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e23adde-e18d-4cb2-b13b-d89cd37b8c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading activations...\n",
      "Train: 101618, Val: 21775, Test: 21776\n",
      "Labels aligned: train=101618, val=21775, test=21776\n",
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/xgboost/callback.py:386: UserWarning: [19:36:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.61332\tval-logloss:0.61343\n",
      "[10]\ttrain-logloss:0.51912\tval-logloss:0.51981\n",
      "[20]\ttrain-logloss:0.47051\tval-logloss:0.47197\n",
      "[30]\ttrain-logloss:0.44107\tval-logloss:0.44347\n",
      "[40]\ttrain-logloss:0.42178\tval-logloss:0.42523\n",
      "[50]\ttrain-logloss:0.40858\tval-logloss:0.41337\n",
      "[60]\ttrain-logloss:0.39670\tval-logloss:0.40350\n",
      "[70]\ttrain-logloss:0.38598\tval-logloss:0.39530\n",
      "[80]\ttrain-logloss:0.37614\tval-logloss:0.38835\n",
      "[90]\ttrain-logloss:0.36761\tval-logloss:0.38272\n",
      "[100]\ttrain-logloss:0.36007\tval-logloss:0.37813\n",
      "[110]\ttrain-logloss:0.35335\tval-logloss:0.37439\n",
      "[120]\ttrain-logloss:0.34740\tval-logloss:0.37135\n",
      "[130]\ttrain-logloss:0.34165\tval-logloss:0.36833\n",
      "[140]\ttrain-logloss:0.33662\tval-logloss:0.36594\n",
      "[150]\ttrain-logloss:0.33178\tval-logloss:0.36386\n",
      "[160]\ttrain-logloss:0.32767\tval-logloss:0.36227\n",
      "[170]\ttrain-logloss:0.32306\tval-logloss:0.36056\n",
      "[180]\ttrain-logloss:0.31894\tval-logloss:0.35915\n",
      "[190]\ttrain-logloss:0.31523\tval-logloss:0.35794\n",
      "[200]\ttrain-logloss:0.31170\tval-logloss:0.35660\n",
      "[210]\ttrain-logloss:0.30809\tval-logloss:0.35531\n",
      "[220]\ttrain-logloss:0.30487\tval-logloss:0.35442\n",
      "[230]\ttrain-logloss:0.30156\tval-logloss:0.35349\n",
      "[240]\ttrain-logloss:0.29832\tval-logloss:0.35268\n",
      "[250]\ttrain-logloss:0.29530\tval-logloss:0.35196\n",
      "[260]\ttrain-logloss:0.29250\tval-logloss:0.35127\n",
      "[270]\ttrain-logloss:0.28960\tval-logloss:0.35055\n",
      "[280]\ttrain-logloss:0.28664\tval-logloss:0.34985\n",
      "[290]\ttrain-logloss:0.28411\tval-logloss:0.34922\n",
      "[300]\ttrain-logloss:0.28174\tval-logloss:0.34871\n",
      "[310]\ttrain-logloss:0.27917\tval-logloss:0.34824\n",
      "[320]\ttrain-logloss:0.27646\tval-logloss:0.34776\n",
      "[330]\ttrain-logloss:0.27408\tval-logloss:0.34712\n",
      "[340]\ttrain-logloss:0.27159\tval-logloss:0.34672\n",
      "[350]\ttrain-logloss:0.26930\tval-logloss:0.34630\n",
      "[360]\ttrain-logloss:0.26655\tval-logloss:0.34592\n",
      "[370]\ttrain-logloss:0.26423\tval-logloss:0.34565\n",
      "[380]\ttrain-logloss:0.26185\tval-logloss:0.34545\n",
      "[390]\ttrain-logloss:0.25949\tval-logloss:0.34518\n",
      "[400]\ttrain-logloss:0.25714\tval-logloss:0.34472\n",
      "[410]\ttrain-logloss:0.25485\tval-logloss:0.34434\n",
      "[420]\ttrain-logloss:0.25265\tval-logloss:0.34380\n",
      "[430]\ttrain-logloss:0.25060\tval-logloss:0.34346\n",
      "[440]\ttrain-logloss:0.24866\tval-logloss:0.34336\n",
      "[450]\ttrain-logloss:0.24664\tval-logloss:0.34316\n",
      "[460]\ttrain-logloss:0.24457\tval-logloss:0.34306\n",
      "[470]\ttrain-logloss:0.24233\tval-logloss:0.34268\n",
      "[480]\ttrain-logloss:0.24062\tval-logloss:0.34267\n",
      "[490]\ttrain-logloss:0.23847\tval-logloss:0.34243\n",
      "[500]\ttrain-logloss:0.23650\tval-logloss:0.34232\n",
      "[510]\ttrain-logloss:0.23422\tval-logloss:0.34226\n",
      "[520]\ttrain-logloss:0.23222\tval-logloss:0.34213\n",
      "[530]\ttrain-logloss:0.23031\tval-logloss:0.34204\n",
      "[540]\ttrain-logloss:0.22850\tval-logloss:0.34187\n",
      "[550]\ttrain-logloss:0.22665\tval-logloss:0.34170\n",
      "[560]\ttrain-logloss:0.22499\tval-logloss:0.34161\n",
      "[570]\ttrain-logloss:0.22324\tval-logloss:0.34142\n",
      "[580]\ttrain-logloss:0.22150\tval-logloss:0.34134\n",
      "[590]\ttrain-logloss:0.21976\tval-logloss:0.34104\n",
      "[600]\ttrain-logloss:0.21826\tval-logloss:0.34097\n",
      "[610]\ttrain-logloss:0.21671\tval-logloss:0.34078\n",
      "[620]\ttrain-logloss:0.21498\tval-logloss:0.34060\n",
      "[630]\ttrain-logloss:0.21328\tval-logloss:0.34047\n",
      "[640]\ttrain-logloss:0.21171\tval-logloss:0.34032\n",
      "[650]\ttrain-logloss:0.21007\tval-logloss:0.34025\n",
      "[660]\ttrain-logloss:0.20833\tval-logloss:0.34012\n",
      "[670]\ttrain-logloss:0.20688\tval-logloss:0.34008\n",
      "[680]\ttrain-logloss:0.20541\tval-logloss:0.34009\n",
      "[683]\ttrain-logloss:0.20502\tval-logloss:0.34010\n",
      "\n",
      "Training complete. Best iteration: 663\n",
      "Best validation logloss: 0.3400\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.8415\n",
      "Precision: 0.8194\n",
      "Recall:    0.6509\n",
      "F1 Score:  0.7255\n",
      "AUC:       0.9104\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8491    0.9320    0.8886     14769\n",
      "           1     0.8194    0.6509    0.7255      7007\n",
      "\n",
      "    accuracy                         0.8415     21776\n",
      "   macro avg     0.8343    0.7914    0.8071     21776\n",
      "weighted avg     0.8396    0.8415    0.8361     21776\n",
      "\n",
      "============================================================\n",
      "Model saved to hallucination_probe_layer_16.pkl\n",
      "\n",
      "Probe saved!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from llmscan import XGBoostProbe\n",
    "\n",
    "# Load cached activations\n",
    "print(\"Loading activations...\")\n",
    "train_acts = torch.load(\"feature_cache16/train_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "val_acts = torch.load(\"feature_cache16/val_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "test_acts = torch.load(\"feature_cache16/test_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "\n",
    "print(f\"Train: {len(train_acts)}, Val: {len(val_acts)}, Test: {len(test_acts)}\")\n",
    "\n",
    "# Align labels (trim to match activations)\n",
    "train_labels_aligned = train_labels[:len(train_acts)]\n",
    "val_labels_aligned = val_labels[:len(val_acts)]\n",
    "test_labels_aligned = test_labels[:len(test_acts)]\n",
    "\n",
    "print(f\"Labels aligned: train={len(train_labels_aligned)}, val={len(val_labels_aligned)}, test={len(test_labels_aligned)}\")\n",
    "\n",
    "# XGBoost params\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'eval_metric': 'logloss',\n",
    "}\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "probe = XGBoostProbe(xgb_params=XGB_PARAMS)\n",
    "probe.fit(\n",
    "    train_acts,\n",
    "    train_labels_aligned,\n",
    "    X_val=val_acts,\n",
    "    y_val=val_labels_aligned,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "metrics = probe.evaluate(test_acts, test_labels_aligned, verbose=True)\n",
    "\n",
    "# Save probe\n",
    "probe.save(\"hallucination_probe_layer_16.pkl\")\n",
    "print(\"\\nProbe saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc38d8-fe0a-40f8-a0cc-899397e35318",
   "metadata": {},
   "source": [
    "### 5. Comments\n",
    "\n",
    "We have trained an XGBoost probe on activation layer 16, previously identified as the most expressive layer for hallucination detection. At the default decision threshold of 0.5, the probe achieves 84.15% accuracy, 91.04% AUC, 81.94% precision, 65.09% recall, and a 72.55% F1 score.\r\n",
    "\r\n",
    "These results indicate that the selected layer's activations encode substantial information relevant to hallucination detection, supporting the hypothesis that\r\n",
    "\r\n",
    "<p style=\"text-align: center;\"><i>hallucination-related signals are present and extractable from the model's internal representations</i>.</p>\r\n",
    "\r\n",
    "However, an asymmetry in per-class performance is apparent: while non-hallucinated responses (class `0`) are identified with high recall (93.2%), the probe captures only 65.1% of actual hallucinations (class `1`). This conservative behaviour (where the classifier is more likely to miss a hallucination than to falsely flag a correct response) suggests that the default threshold may not be optimal for our class distribution.\r\n",
    "\r\n",
    "The strong AUC of 91% indicates that better operating points likely exist along the precision-recall curve. Before drawing broader conclusions, we therefore investigate whether adjusting the decision threshold can improve hallucination recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29976d88-8a7f-4a21-b4ca-4390a3228f40",
   "metadata": {},
   "source": [
    "### 6. Threshold optimization\n",
    "\n",
    "Let's see how the XGBoost threshold can be tuned to optimize performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd2c65b6-6bad-4484-8b6c-f7c4da79d2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from hallucination_probe_layer_16.pkl\n",
      "Best threshold: 0.366\n",
      "Precision: 0.725, Recall: 0.759, F1: 0.741\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8829    0.8632    0.8730     14769\n",
      "           1     0.7246    0.7587    0.7413      7007\n",
      "\n",
      "    accuracy                         0.8296     21776\n",
      "   macro avg     0.8038    0.8109    0.8071     21776\n",
      "weighted avg     0.8320    0.8296    0.8306     21776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from llmscan import XGBoostProbe\n",
    "\n",
    "# Load model using the class method\n",
    "probe = XGBoostProbe.load(\"hallucination_probe_layer_16.pkl\")\n",
    "\n",
    "# Get probabilities for hallucination class\n",
    "y_proba = probe.predict_proba(test_acts)[:, 1]\n",
    "\n",
    "# Get precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels_aligned, y_proba)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n",
    "\n",
    "# Full report with optimized threshold\n",
    "y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "print(\"\\nClassification Report (optimized threshold):\")\n",
    "print(classification_report(test_labels_aligned, y_pred_optimized, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ba0bd3-f689-479f-a26e-594725b70c85",
   "metadata": {},
   "source": [
    "Threshold tuning confirms that a better operating point was indeed available. By lowering the threshold from 0.5 to the optimal value, hallucination recall improves from approximately 65% to 76%, at the cost of a modest increase in false positives - a reasonable tradeoff for hallucination detection, where missing a hallucination is typically more costly than occasionally flagging a correct response.\n",
    "\n",
    "Overall accuracy decreases marginally, but F1 improves. The default 0.5 threshold was evidently too conservative given the dataset's class distribution (~2:1 ratio favouring non-hallucinations), which reflects the model's actual hallucination rate on the evaluated tasks rather than an artifact of data collection.\n",
    "\n",
    "### 7. Conclusion\n",
    "\n",
    "These results establish that hallucination-relevant signal is extractable from layer 16's activations using a lightweight XGBoost classifier, achieving 91% AUC and, after threshold optimisation, a 74% F1 score with 76% recall on the hallucination class.\n",
    "\n",
    "Several directions warrant further investigation:\n",
    "\n",
    "- *Multi-layer probing:* Combining activations from multiple layers may capture complementary signals, with early layers potentially encoding input-related uncertainty and later layers encoding output confidence.\n",
    "- *Attention features:* Incorporating attention patterns or per-head statistics could provide additional discriminative information beyond the feed-forward activations.\n",
    "- *Feature analysis:* Examining which activation dimensions drive probe decisions could offer interpretability insights into how the model internally represents uncertainty or fabrication.\n",
    "\n",
    "The following notebooks will explore multi-layer concatenation and attention-based features, starting with the former."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
