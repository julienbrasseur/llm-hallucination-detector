{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4935367-9438-45d6-a3b2-c45d4d881ea1",
   "metadata": {},
   "source": [
    "# **Second experiment: training a XGBoost probe on multiple activation layers**\n",
    "\n",
    "Having trained a probe on the single best-performing layer in the previous notebook, we now investigate whether combining multiple layers yields improved performance. We begin with layers 15 and 16 (the two top-performing layers as identified in our initial layer selection analysis) and subsequently extend our analysis to include layer 18.\n",
    "\n",
    "For each configuration, we explore two approaches:\n",
    "- Direct concatenation of pooled activations;\n",
    "- Feature selection, retaining varying percentages of the most informative features.\n",
    "\n",
    "The training procedure follows the methodology established in the previous notebook.\n",
    "\n",
    "*Prerequisite:* This notebook assumes that activations for layers 15 and 18 have already been extracted following the procedure detailed in the previous notebook for layer 16. If this has not yet been done, the extraction process is entirely analogous and should be completed before proceeding.\n",
    "\n",
    "### 1. Installing required libraries\n",
    "\n",
    "We begin by installing the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6b92b46-5f79-4eb0-bdd8-3703c53311d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/julienbrasseur/llm-hallucination-detector.git\n",
      "  Cloning https://github.com/julienbrasseur/llm-hallucination-detector.git to /tmp/pip-req-build-z_gkm7qp\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/julienbrasseur/llm-hallucination-detector.git /tmp/pip-req-build-z_gkm7qp\n",
      "  Resolved https://github.com/julienbrasseur/llm-hallucination-detector.git to commit 77b721d351f3cb5b08d8447d199d6afe38970d26\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from llmscan==0.1.0) (2.4.1+cu124)\n",
      "Collecting transformers>=4.36.0 (from llmscan==0.1.0)\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting xgboost>=2.0.0 (from llmscan==0.1.0)\n",
      "  Downloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting scikit-learn>=1.3.0 (from llmscan==0.1.0)\n",
      "  Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.0 in /usr/local/lib/python3.11/dist-packages (from llmscan==0.1.0) (1.26.3)\n",
      "Collecting scipy>=1.11.0 (from llmscan==0.1.0)\n",
      "  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n",
      "Collecting tqdm>=4.65.0 (from llmscan==0.1.0)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting accelerate>=0.25.0 (from llmscan==0.1.0)\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting matplotlib>=3.7.0 (from llmscan==0.1.0)\n",
      "  Downloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (52 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (24.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.25.0->llmscan==0.1.0) (6.0.2)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading huggingface_hub-1.2.3-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors>=0.4.3 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (2024.2.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (0.27.2)\n",
      "Collecting shellingham (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading typer_slim-0.20.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (4.9.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0) (0.14.0)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llmscan==0.1.0) (10.2.0)\n",
      "Collecting pyparsing>=3 (from matplotlib>=3.7.0->llmscan==0.1.0)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.7.0->llmscan==0.1.0) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=3.7.0->llmscan==0.1.0) (1.16.0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn>=1.3.0->llmscan==0.1.0)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn>=1.3.0->llmscan==0.1.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.2.65 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.2.65)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.0.44 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (11.2.0.44)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.119 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (10.3.5.119)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.0.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (11.6.0.99)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.0.142 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.3.0.142)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.99 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (12.4.99)\n",
      "Requirement already satisfied: triton==3.0.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->llmscan==0.1.0) (3.0.0)\n",
      "Collecting huggingface_hub>=0.21.0 (from accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.36.0->llmscan==0.1.0)\n",
      "  Downloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.36.0->llmscan==0.1.0) (2.32.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers>=4.36.0->llmscan==0.1.0)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->llmscan==0.1.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->llmscan==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.36.0->llmscan==0.1.0) (2.2.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy->torch>=2.0.0->llmscan==0.1.0) (1.3.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub>=0.21.0->accelerate>=0.25.0->llmscan==0.1.0)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading matplotlib-3.10.8-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (355 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Downloading safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Downloading scikit_learn-1.8.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (9.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m141.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m173.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m203.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2025.11.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (800 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.4/800.4 kB\u001b[0m \u001b[31m129.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading xgboost-3.1.2-py3-none-manylinux_2_28_x86_64.whl (115.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.9/115.9 MB\u001b[0m \u001b[31m131.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: llmscan\n",
      "  Building wheel for llmscan (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llmscan: filename=llmscan-0.1.0-py3-none-any.whl size=18859 sha256=be2c10c7d081a7cd1439063f1a247341e142931328baa8ffdaa7e71827d5c095\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-ifzgpvip/wheels/eb/e6/51/577f098f1ff2729ce5349f16e3327eba87b9c2b9d3ea4df270\n",
      "Successfully built llmscan\n",
      "Installing collected packages: tqdm, threadpoolctl, scipy, safetensors, regex, pyparsing, kiwisolver, joblib, hf-xet, fonttools, cycler, contourpy, xgboost, scikit-learn, matplotlib, huggingface_hub, tokenizers, transformers, accelerate, llmscan\n",
      "\u001b[2K  Attempting uninstall: pyparsing━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K    Found existing installation: pyparsing 2.4.7━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K    Uninstalling pyparsing-2.4.7:━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K      Successfully uninstalled pyparsing-2.4.7━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 2/20\u001b[0m [scipy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [llmscan]8/20\u001b[0m [accelerate]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.12.0 contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 hf-xet-1.2.0 huggingface_hub-0.36.0 joblib-1.5.3 kiwisolver-1.4.9 llmscan-0.1.0 matplotlib-3.10.8 pyparsing-3.2.5 regex-2025.11.3 safetensors-0.7.0 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.3 xgboost-3.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting datasets\n",
      "  Downloading datasets-4.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.36.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (4.6.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-4.4.2-py3-none-any.whl (512 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Downloading multiprocess-0.70.18-py311-none-any.whl (144 kB)\n",
      "Downloading aiohttp-3.13.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading multidict-6.7.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (246 kB)\n",
      "Downloading yarl-1.22.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (365 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (231 kB)\n",
      "Downloading propcache-0.4.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (210 kB)\n",
      "Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-2.3.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
      "Downloading xxhash-3.6.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, pandas, multiprocess, aiosignal, aiohttp, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15/15\u001b[0m [datasets]/15\u001b[0m [datasets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.2 dill-0.4.0 frozenlist-1.8.0 multidict-6.7.0 multiprocess-0.70.18 pandas-2.3.3 propcache-0.4.1 pyarrow-22.0.0 pytz-2025.2 tzdata-2025.3 xxhash-3.6.0 yarl-1.22.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install `llmscan`\n",
    "!pip install git+https://github.com/julienbrasseur/llm-hallucination-detector.git\n",
    "\n",
    "# Install `datasets`\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b3029-5dbb-457a-a1e8-858495aab50b",
   "metadata": {},
   "source": [
    "### 2. Data preparation\n",
    "\n",
    "As before, we load the dataset from Hugging Face and convert it to the standard OpenAI conversation format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfbcbf0e-27c4-48d7-a9be-78c0de37c688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset: krogoldAI/hallucination-labeled-dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf857269c414dadb2cabfd81e81309d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f812786eb6214baf82dac9112700f643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/78.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "439d448503b44d319d723dfec25560d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/validation-00000-of-00001.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56855edcdec64bc38a974819f6f24c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc72bf0baef4056a9107c27d11fb2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/101618 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84ebb44823a47b0adb684856a35d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/21775 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b04bc51dd7f84b73aa0701e92efc3fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/21776 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded and formatted:\n",
      "  Train:      101,618 examples\n",
      "  Validation: 21,775 examples\n",
      "  Test:       21,776 examples\n",
      "  Class distribution (train): 68,913 non-hallucination, 32,705 hallucination\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set training dataset path\n",
    "DATASET_NAME = \"krogoldAI/hallucination-labeled-dataset\"\n",
    "\n",
    "def load_and_format_dataset(dataset_name: str):\n",
    "    \"\"\"\n",
    "    Load HuggingFace dataset and convert to conversation format.\n",
    "\n",
    "    This function converts dataset with 'input', 'target', 'hallucination' fields\n",
    "    to the standard conversation format expected by the pipeline.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_data, val_data, test_data, train_labels, val_labels, test_labels)\n",
    "    \"\"\"\n",
    "    print(f\"Loading dataset: {dataset_name}\")\n",
    "    ds = load_dataset(dataset_name)\n",
    "\n",
    "    # Shuffle each split\n",
    "    ds[\"train\"] = ds[\"train\"].shuffle(seed=42)\n",
    "    ds[\"validation\"] = ds[\"validation\"].shuffle(seed=42)\n",
    "    ds[\"test\"] = ds[\"test\"].shuffle(seed=42)\n",
    "\n",
    "    def format_split(split):\n",
    "        \"\"\"Convert HF dataset split to conversation format.\"\"\"\n",
    "        formatted = []\n",
    "        labels = []\n",
    "\n",
    "        for item in split:\n",
    "            # Extract fields from your HF dataset format\n",
    "            user_msg = item[\"input\"]\n",
    "            assistant_msg = item[\"target\"]\n",
    "            label = int(item[\"hallucination\"])\n",
    "\n",
    "            # Convert to standard conversation format\n",
    "            formatted.append({\n",
    "                \"conversation\": [\n",
    "                    {\"role\": \"user\", \"content\": user_msg},\n",
    "                    {\"role\": \"assistant\", \"content\": assistant_msg},\n",
    "                ]\n",
    "            })\n",
    "            labels.append(label)\n",
    "\n",
    "        return formatted, np.array(labels)\n",
    "\n",
    "    # Format all splits\n",
    "    train_data, train_labels = format_split(ds[\"train\"])\n",
    "    val_data, val_labels = format_split(ds[\"validation\"])\n",
    "    test_data, test_labels = format_split(ds[\"test\"])\n",
    "\n",
    "    print(f\"Dataset loaded and formatted:\")\n",
    "    print(f\"  Train:      {len(train_data):,} examples\")\n",
    "    print(f\"  Validation: {len(val_data):,} examples\")\n",
    "    print(f\"  Test:       {len(test_data):,} examples\")\n",
    "    print(f\"  Class distribution (train): \"\n",
    "          f\"{(train_labels == 0).sum():,} non-hallucination, \"\n",
    "          f\"{(train_labels == 1).sum():,} hallucination\")\n",
    "\n",
    "    return train_data, val_data, test_data, train_labels, val_labels, test_labels\n",
    "\n",
    "# Load and format dataset\n",
    "train_data, val_data, test_data, train_labels, val_labels, test_labels = \\\n",
    "    load_and_format_dataset(DATASET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab09c88e-48a5-4605-a5c8-35a53d2f8c40",
   "metadata": {},
   "source": [
    "### 3. Reloading activations\n",
    "\n",
    "We load the pre-extracted activations for layers 15 and 16, concatenate them, and align the resulting arrays with the binary hallucination labels from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebb13746-0d47-4a14-841f-be809c17f607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading activations for layer 15...\n",
      "Layer 15 - Train: (101618, 4096), Val: (21775, 4096), Test: (21776, 4096)\n",
      "Loading activations for layer 16...\n",
      "Layer 16 - Train: (101618, 4096), Val: (21775, 4096), Test: (21776, 4096)\n",
      "\n",
      "Concatenated shapes:\n",
      "  Train: (101618, 8192)\n",
      "  Val:   (21775, 8192)\n",
      "  Test:  (21776, 8192)\n",
      "\n",
      "Labels aligned:\n",
      "  Train: 101618\n",
      "  Val:   21775\n",
      "  Test:  21776\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load activations for layer 15\n",
    "print(\"Loading activations for layer 15...\")\n",
    "train_acts_15 = torch.load(\"/workspace/feature_cache15/train_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "val_acts_15 = torch.load(\"/workspace/feature_cache15/val_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "test_acts_15 = torch.load(\"/workspace/feature_cache15/test_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "print(f\"Layer 15 - Train: {train_acts_15.shape}, Val: {val_acts_15.shape}, Test: {test_acts_15.shape}\")\n",
    "\n",
    "# Load activations for layer 16\n",
    "print(\"Loading activations for layer 16...\")\n",
    "train_acts_16 = torch.load(\"/workspace/feature_cache16/train_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "val_acts_16 = torch.load(\"/workspace/feature_cache16/val_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "test_acts_16 = torch.load(\"/workspace/feature_cache16/test_activations_pooled.pt\", map_location=\"cpu\", weights_only=True).float().numpy()\n",
    "print(f\"Layer 16 - Train: {train_acts_16.shape}, Val: {val_acts_16.shape}, Test: {test_acts_16.shape}\")\n",
    "\n",
    "# Concatenate along feature dimension (axis=1)\n",
    "train_acts = np.concatenate([train_acts_15, train_acts_16], axis=1)\n",
    "val_acts = np.concatenate([val_acts_15, val_acts_16], axis=1)\n",
    "test_acts = np.concatenate([test_acts_15, test_acts_16], axis=1)\n",
    "\n",
    "print(f\"\\nConcatenated shapes:\")\n",
    "print(f\"  Train: {train_acts.shape}\")  # Should be [N, 4096*2] = [N, 8192]\n",
    "print(f\"  Val:   {val_acts.shape}\")\n",
    "print(f\"  Test:  {test_acts.shape}\")\n",
    "\n",
    "# Align labels (use minimum length in case of mismatch)\n",
    "min_train = min(len(train_acts_15), len(train_acts_16))\n",
    "min_val = min(len(val_acts_15), len(val_acts_16))\n",
    "min_test = min(len(test_acts_15), len(test_acts_16))\n",
    "\n",
    "train_labels_aligned = train_labels[:min_train]\n",
    "val_labels_aligned = val_labels[:min_val]\n",
    "test_labels_aligned = test_labels[:min_test]\n",
    "\n",
    "print(f\"\\nLabels aligned:\")\n",
    "print(f\"  Train: {len(train_labels_aligned)}\")\n",
    "print(f\"  Val:   {len(val_labels_aligned)}\")\n",
    "print(f\"  Test:  {len(test_labels_aligned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "410b6c19-50e5-4e08-b89a-dfe6ef6da265",
   "metadata": {},
   "source": [
    "### 4. Training a XGBoost probe on concatenated layers 15 and 16\n",
    "\n",
    "With the data prepared, we proceed to train an XGBoost probe on the concatenated activations, following the same procedure as for the single-layer case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5e1056c-0b88-4f65-9270-d6f73e565046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:47:02] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.61331\tval-logloss:0.61352\n",
      "[10]\ttrain-logloss:0.51802\tval-logloss:0.52016\n",
      "[20]\ttrain-logloss:0.46693\tval-logloss:0.47092\n",
      "[30]\ttrain-logloss:0.43693\tval-logloss:0.44245\n",
      "[40]\ttrain-logloss:0.41777\tval-logloss:0.42460\n",
      "[50]\ttrain-logloss:0.40332\tval-logloss:0.41179\n",
      "[60]\ttrain-logloss:0.39077\tval-logloss:0.40171\n",
      "[70]\ttrain-logloss:0.38010\tval-logloss:0.39401\n",
      "[80]\ttrain-logloss:0.37074\tval-logloss:0.38782\n",
      "[90]\ttrain-logloss:0.36205\tval-logloss:0.38249\n",
      "[100]\ttrain-logloss:0.35410\tval-logloss:0.37800\n",
      "[110]\ttrain-logloss:0.34719\tval-logloss:0.37441\n",
      "[120]\ttrain-logloss:0.34140\tval-logloss:0.37144\n",
      "[130]\ttrain-logloss:0.33583\tval-logloss:0.36881\n",
      "[140]\ttrain-logloss:0.33080\tval-logloss:0.36651\n",
      "[150]\ttrain-logloss:0.32592\tval-logloss:0.36453\n",
      "[160]\ttrain-logloss:0.32177\tval-logloss:0.36299\n",
      "[170]\ttrain-logloss:0.31740\tval-logloss:0.36156\n",
      "[180]\ttrain-logloss:0.31339\tval-logloss:0.36016\n",
      "[190]\ttrain-logloss:0.30940\tval-logloss:0.35911\n",
      "[200]\ttrain-logloss:0.30574\tval-logloss:0.35790\n",
      "[210]\ttrain-logloss:0.30213\tval-logloss:0.35667\n",
      "[220]\ttrain-logloss:0.29894\tval-logloss:0.35553\n",
      "[230]\ttrain-logloss:0.29578\tval-logloss:0.35480\n",
      "[240]\ttrain-logloss:0.29236\tval-logloss:0.35410\n",
      "[250]\ttrain-logloss:0.28896\tval-logloss:0.35352\n",
      "[260]\ttrain-logloss:0.28594\tval-logloss:0.35280\n",
      "[270]\ttrain-logloss:0.28319\tval-logloss:0.35218\n",
      "[280]\ttrain-logloss:0.28013\tval-logloss:0.35171\n",
      "[290]\ttrain-logloss:0.27714\tval-logloss:0.35111\n",
      "[300]\ttrain-logloss:0.27430\tval-logloss:0.35062\n",
      "[310]\ttrain-logloss:0.27163\tval-logloss:0.35017\n",
      "[320]\ttrain-logloss:0.26888\tval-logloss:0.34964\n",
      "[330]\ttrain-logloss:0.26605\tval-logloss:0.34884\n",
      "[340]\ttrain-logloss:0.26327\tval-logloss:0.34835\n",
      "[350]\ttrain-logloss:0.26058\tval-logloss:0.34797\n",
      "[360]\ttrain-logloss:0.25812\tval-logloss:0.34764\n",
      "[370]\ttrain-logloss:0.25560\tval-logloss:0.34709\n",
      "[380]\ttrain-logloss:0.25319\tval-logloss:0.34684\n",
      "[390]\ttrain-logloss:0.25058\tval-logloss:0.34642\n",
      "[400]\ttrain-logloss:0.24820\tval-logloss:0.34608\n",
      "[410]\ttrain-logloss:0.24592\tval-logloss:0.34561\n",
      "[420]\ttrain-logloss:0.24369\tval-logloss:0.34538\n",
      "[430]\ttrain-logloss:0.24175\tval-logloss:0.34515\n",
      "[440]\ttrain-logloss:0.23974\tval-logloss:0.34491\n",
      "[450]\ttrain-logloss:0.23762\tval-logloss:0.34447\n",
      "[460]\ttrain-logloss:0.23573\tval-logloss:0.34418\n",
      "[470]\ttrain-logloss:0.23360\tval-logloss:0.34406\n",
      "[480]\ttrain-logloss:0.23168\tval-logloss:0.34391\n",
      "[490]\ttrain-logloss:0.22960\tval-logloss:0.34369\n",
      "[500]\ttrain-logloss:0.22771\tval-logloss:0.34349\n",
      "[510]\ttrain-logloss:0.22577\tval-logloss:0.34333\n",
      "[520]\ttrain-logloss:0.22396\tval-logloss:0.34314\n",
      "[530]\ttrain-logloss:0.22196\tval-logloss:0.34294\n",
      "[540]\ttrain-logloss:0.21995\tval-logloss:0.34278\n",
      "[550]\ttrain-logloss:0.21807\tval-logloss:0.34263\n",
      "[560]\ttrain-logloss:0.21634\tval-logloss:0.34240\n",
      "[570]\ttrain-logloss:0.21467\tval-logloss:0.34231\n",
      "[580]\ttrain-logloss:0.21276\tval-logloss:0.34226\n",
      "[590]\ttrain-logloss:0.21096\tval-logloss:0.34203\n",
      "[600]\ttrain-logloss:0.20932\tval-logloss:0.34188\n",
      "[610]\ttrain-logloss:0.20772\tval-logloss:0.34190\n",
      "[620]\ttrain-logloss:0.20609\tval-logloss:0.34161\n",
      "[630]\ttrain-logloss:0.20438\tval-logloss:0.34152\n",
      "[640]\ttrain-logloss:0.20254\tval-logloss:0.34114\n",
      "[650]\ttrain-logloss:0.20086\tval-logloss:0.34082\n",
      "[660]\ttrain-logloss:0.19928\tval-logloss:0.34075\n",
      "[670]\ttrain-logloss:0.19785\tval-logloss:0.34065\n",
      "[680]\ttrain-logloss:0.19613\tval-logloss:0.34080\n",
      "[690]\ttrain-logloss:0.19469\tval-logloss:0.34083\n",
      "\n",
      "Training complete. Best iteration: 670\n",
      "Best validation logloss: 0.3406\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.8425\n",
      "Precision: 0.8236\n",
      "Recall:    0.6498\n",
      "F1 Score:  0.7264\n",
      "AUC:       0.9099\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8490    0.9340    0.8894     14769\n",
      "           1     0.8236    0.6498    0.7264      7007\n",
      "\n",
      "    accuracy                         0.8425     21776\n",
      "   macro avg     0.8363    0.7919    0.8079     21776\n",
      "weighted avg     0.8408    0.8425    0.8370     21776\n",
      "\n",
      "============================================================\n",
      "Model saved to hallucination_probe_layers_15_16.pkl\n",
      "\n",
      "Probe saved!\n"
     ]
    }
   ],
   "source": [
    "from llmscan import XGBoostProbe\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'eval_metric': 'logloss',\n",
    "}\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "probe = XGBoostProbe(xgb_params=XGB_PARAMS)\n",
    "probe.fit(\n",
    "    train_acts,\n",
    "    train_labels_aligned,\n",
    "    X_val=val_acts,\n",
    "    y_val=val_labels_aligned,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "metrics = probe.evaluate(test_acts, test_labels_aligned, verbose=True)\n",
    "\n",
    "# Save probe\n",
    "probe.save(\"hallucination_probe_layers_15_16.pkl\")\n",
    "print(\"\\nProbe saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860f5b30-daa3-4f6f-9782-c46b42b766fd",
   "metadata": {},
   "source": [
    "As in the previous notebook, we tune the decision threshold to optimise the F1 score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da37240d-bf75-4dd5-944c-c3f7eb9b355a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from hallucination_probe_layers_15_16.pkl\n",
      "Best threshold: 0.388\n",
      "Precision: 0.749, Recall: 0.744, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8788    0.8821    0.8804     14769\n",
      "           1     0.7494    0.7435    0.7465      7007\n",
      "\n",
      "    accuracy                         0.8375     21776\n",
      "   macro avg     0.8141    0.8128    0.8134     21776\n",
      "weighted avg     0.8372    0.8375    0.8373     21776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "from llmscan import XGBoostProbe\n",
    "\n",
    "# Load model using the class method\n",
    "probe = XGBoostProbe.load(\"hallucination_probe_layers_15_16.pkl\")\n",
    "\n",
    "# Get probabilities for hallucination class\n",
    "y_proba = probe.predict_proba(test_acts)[:, 1]\n",
    "\n",
    "# Get precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels_aligned, y_proba)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n",
    "\n",
    "# Full report with optimized threshold\n",
    "y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "print(\"\\nClassification Report (optimized threshold):\")\n",
    "print(classification_report(test_labels_aligned, y_pred_optimized, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c866051-857d-4960-a5eb-d8f27ae63264",
   "metadata": {},
   "source": [
    "### 5. Comment\n",
    "\n",
    "After threshold optimisation, performance remains comparable to the single best-performing layer. While accuracy shows a marginal improvement of approximately 1 percentage point (~84%), precision, recall, and F1 score exhibit only minor variations that may well fall within the range of experimental noise.\n",
    "\n",
    "This outcome suggests that concatenating additional layers may not yield substantial benefits, particularly when weighed against the increased computational cost of extracting and processing multiple activation layers. Given that the remaining layers are less expressive for hallucination detection than layers 15 and 16, further layer additions would likely produce diminishing returns.\n",
    "\n",
    "Nevertheless, it remains possible that the concatenated representation contains redundant dimensions. Feature selection may both reduce computational overhead and potentially improve generalisation by eliminating noise. Since this analysis is computationally inexpensive, we proceed to investigate.\n",
    "\n",
    "### 6. Feature selection experiment\n",
    "\n",
    "We now train XGBoost probes on subsets of the most important features, ranging from 500 to 4,000 features in increments of 500. If no significant improvement emerges within this range, there is little justification for extending the search further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "447df794-af4e-4359-bc93-4573c08ee91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================\n",
      "FEATURE SELECTION EXPERIMENT\n",
      "===============================================================\n",
      "\n",
      "Extracting feature importance...\n",
      "  Total features: 7866\n",
      "  Features with non-zero importance: 7866\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 500 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 500 features (indices: [5125, 818, 4806, 3395, 1845]...[2326, 4239, 6170, 62, 3169])\n",
      "Subset shapes: (101618, 500), (21775, 500), (21776, 500)\n",
      "Training XGBoost on 500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:48:04] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 738)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8289, Recall=0.7433, F1=0.7366, AUC=0.9067\n",
      "Best threshold: 0.419\n",
      "Precision: 0.763, Recall: 0.718, F1: 0.740\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8698    0.8941    0.8818     14769\n",
      "           1     0.7628    0.7179    0.7397      7007\n",
      "\n",
      "    accuracy                         0.8374     21776\n",
      "   macro avg     0.8163    0.8060    0.8107     21776\n",
      "weighted avg     0.8354    0.8374    0.8360     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 1000 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 1000 features (indices: [5125, 818, 4806, 3395, 1845]...[2331, 5679, 7893, 6367, 1891])\n",
      "Subset shapes: (101618, 1000), (21775, 1000), (21776, 1000)\n",
      "Training XGBoost on 1000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:48:27] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 821)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8351, Recall=0.7467, F1=0.7446, AUC=0.9101\n",
      "Best threshold: 0.371\n",
      "Precision: 0.732, Recall: 0.763, F1: 0.747\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8852    0.8678    0.8764     14769\n",
      "           1     0.7325    0.7628    0.7473      7007\n",
      "\n",
      "    accuracy                         0.8340     21776\n",
      "   macro avg     0.8089    0.8153    0.8119     21776\n",
      "weighted avg     0.8361    0.8340    0.8349     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 1500 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 1500 features (indices: [5125, 818, 4806, 3395, 1845]...[7657, 5145, 6164, 6176, 2436])\n",
      "Subset shapes: (101618, 1500), (21775, 1500), (21776, 1500)\n",
      "Training XGBoost on 1500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:49:12] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 901)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8339, Recall=0.7403, F1=0.7415, AUC=0.9110\n",
      "Best threshold: 0.361\n",
      "Precision: 0.725, Recall: 0.768, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8867    0.8620    0.8742     14769\n",
      "           1     0.7253    0.7679    0.7460      7007\n",
      "\n",
      "    accuracy                         0.8317     21776\n",
      "   macro avg     0.8060    0.8150    0.8101     21776\n",
      "weighted avg     0.8348    0.8317    0.8330     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 2000 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 2000 features (indices: [5125, 818, 4806, 3395, 1845]...[2439, 1905, 6298, 976, 1019])\n",
      "Subset shapes: (101618, 2000), (21775, 2000), (21776, 2000)\n",
      "Training XGBoost on 2000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:50:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 676)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8340, Recall=0.7435, F1=0.7425, AUC=0.9098\n",
      "Best threshold: 0.378\n",
      "Precision: 0.733, Recall: 0.755, F1: 0.744\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8822    0.8697    0.8759     14769\n",
      "           1     0.7333    0.7551    0.7441      7007\n",
      "\n",
      "    accuracy                         0.8328     21776\n",
      "   macro avg     0.8077    0.8124    0.8100     21776\n",
      "weighted avg     0.8343    0.8328    0.8335     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 2500 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 2500 features (indices: [5125, 818, 4806, 3395, 1845]...[3953, 2645, 3742, 315, 1458])\n",
      "Subset shapes: (101618, 2500), (21775, 2500), (21776, 2500)\n",
      "Training XGBoost on 2500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:51:54] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 792)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8363, Recall=0.7418, F1=0.7447, AUC=0.9106\n",
      "Best threshold: 0.371\n",
      "Precision: 0.735, Recall: 0.761, F1: 0.748\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8846    0.8701    0.8773     14769\n",
      "           1     0.7353    0.7607    0.7478      7007\n",
      "\n",
      "    accuracy                         0.8349     21776\n",
      "   macro avg     0.8099    0.8154    0.8125     21776\n",
      "weighted avg     0.8365    0.8349    0.8356     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 3000 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 3000 features (indices: [5125, 818, 4806, 3395, 1845]...[2325, 5225, 5950, 7561, 3741])\n",
      "Subset shapes: (101618, 3000), (21775, 3000), (21776, 3000)\n",
      "Training XGBoost on 3000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:54:09] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 703)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8347, Recall=0.7418, F1=0.7428, AUC=0.9097\n",
      "Best threshold: 0.369\n",
      "Precision: 0.728, Recall: 0.761, F1: 0.744\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8840    0.8655    0.8746     14769\n",
      "           1     0.7284    0.7607    0.7442      7007\n",
      "\n",
      "    accuracy                         0.8317     21776\n",
      "   macro avg     0.8062    0.8131    0.8094     21776\n",
      "weighted avg     0.8340    0.8317    0.8327     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 3500 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 3500 features (indices: [5125, 818, 4806, 3395, 1845]...[6343, 4635, 7609, 6012, 437])\n",
      "Subset shapes: (101618, 3500), (21775, 3500), (21776, 3500)\n",
      "Training XGBoost on 3500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [12:56:45] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 794)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8352, Recall=0.7407, F1=0.7431, AUC=0.9107\n",
      "Best threshold: 0.389\n",
      "Precision: 0.747, Recall: 0.740, F1: 0.744\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8773    0.8810    0.8791     14769\n",
      "           1     0.7469    0.7403    0.7435      7007\n",
      "\n",
      "    accuracy                         0.8357     21776\n",
      "   macro avg     0.8121    0.8106    0.8113     21776\n",
      "weighted avg     0.8353    0.8357    0.8355     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "EXPERIMENT: Top 4000 features\n",
      "===============================================================\n",
      "\n",
      "Selected top 4000 features (indices: [5125, 818, 4806, 3395, 1845]...[1288, 6773, 7171, 4929, 1730])\n",
      "Subset shapes: (101618, 4000), (21775, 4000), (21776, 4000)\n",
      "Training XGBoost on 4000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [13:00:20] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete (best iteration: 699)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8361, Recall=0.7388, F1=0.7437, AUC=0.9109\n",
      "Best threshold: 0.365\n",
      "Precision: 0.729, Recall: 0.763, F1: 0.745\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8848    0.8655    0.8751     14769\n",
      "           1     0.7290    0.7625    0.7454      7007\n",
      "\n",
      "    accuracy                         0.8324     21776\n",
      "   macro avg     0.8069    0.8140    0.8102     21776\n",
      "weighted avg     0.8347    0.8324    0.8333     21776\n",
      "\n",
      "\n",
      "===============================================================\n",
      "FEATURE SELECTION RESULTS COMPARISON\n",
      "===============================================================\n",
      "\n",
      "Features     Accuracy   Precision  Recall     F1         AUC       \n",
      "---------------------------------------------------------------\n",
      "500          0.8289     0.7300     0.7433     0.7366     0.9067    \n",
      "1000         0.8351     0.7424     0.7467     0.7446     0.9101    \n",
      "1500         0.8339     0.7427     0.7403     0.7415     0.9110    \n",
      "2000         0.8340     0.7414     0.7435     0.7425     0.9098    \n",
      "2500         0.8363     0.7476     0.7418     0.7447     0.9106    \n",
      "3000         0.8347     0.7438     0.7418     0.7428     0.9097    \n",
      "3500         0.8352     0.7455     0.7407     0.7431     0.9107    \n",
      "4000         0.8361     0.7487     0.7388     0.7437     0.9109    \n",
      "\n",
      "===============================================================\n",
      "BEST MODELS\n",
      "===============================================================\n",
      "\n",
      "Best F1: 2500 features\n",
      "  Accuracy: 0.8363\n",
      "  Recall:   0.7418\n",
      "  F1:       0.7447\n",
      "  AUC:      0.9106\n",
      "\n",
      "Best AUC: 1500 features\n",
      "  Accuracy: 0.8339\n",
      "  Recall:   0.7403\n",
      "  F1:       0.7415\n",
      "  AUC:      0.9110\n",
      "\n",
      "===============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "===============================================================\n",
      "\n",
      "In top 2500 features:\n",
      "  Layer 15 features: 1220 (48.8%)\n",
      "  Layer 16 features:  1280 (51.2%)\n",
      "\n",
      "Both layers contribute roughly equally.\n",
      "\n",
      "===============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llmscan import XGBoostProbe\n",
    "\n",
    "print(\"=\"*63)\n",
    "print(\"FEATURE SELECTION EXPERIMENT\")\n",
    "print(\"=\"*63)\n",
    "\n",
    "# Set feature selection parameters (from 500 to 4000 with a 500 iteration step)\n",
    "MIN_FEATURES = 500\n",
    "MAX_FEATURES = 4000\n",
    "ITERATION_STEP = 500\n",
    "\n",
    "# Extract feature importance\n",
    "print(\"\\nExtracting feature importance...\")\n",
    "feature_importance = probe.get_feature_importance(\n",
    "    importance_type='gain',\n",
    "    top_k=None\n",
    ")\n",
    "print(f\"  Total features: {len(feature_importance)}\")\n",
    "print(f\"  Features with non-zero importance: {len([v for v in feature_importance.values() if v > 0])}\")\n",
    "\n",
    "# Initialize list to store metrics\n",
    "results = []\n",
    "\n",
    "# Iterating over various feature numbers\n",
    "for top_k in range(MIN_FEATURES, MAX_FEATURES+1, ITERATION_STEP):\n",
    "    print(\"\\n\" + \"=\"*63)\n",
    "    print(f\"EXPERIMENT: Top {top_k} features\")\n",
    "    print(\"=\"*63)\n",
    "\n",
    "    # Get top-k most important feature indices\n",
    "    top_features = sorted(\n",
    "        feature_importance.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    top_indices = [idx for idx, _ in top_features]\n",
    "\n",
    "    print(f\"\\nSelected top {top_k} features (indices: {top_indices[:5]}...{top_indices[-5:]})\")\n",
    "\n",
    "    # Subset the data\n",
    "    train_acts_subset = train_acts[:, top_indices]\n",
    "    val_acts_subset = val_acts[:, top_indices]\n",
    "    test_acts_subset = test_acts[:, top_indices]\n",
    "\n",
    "    print(f\"Subset shapes: {train_acts_subset.shape}, {val_acts_subset.shape}, {test_acts_subset.shape}\")\n",
    "\n",
    "    # Train new XGBoost on selected features\n",
    "    print(f\"Training XGBoost on {top_k} selected features...\")\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss',\n",
    "    }\n",
    "\n",
    "    probe_subset = XGBoostProbe(xgb_params=XGB_PARAMS)\n",
    "    probe_subset.fit(\n",
    "        train_acts_subset,\n",
    "        train_labels_aligned,\n",
    "        X_val=val_acts_subset,\n",
    "        y_val=val_labels_aligned,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"Training complete (best iteration: {probe_subset.model.best_iteration})\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(f\"Evaluating...\")\n",
    "    metrics = probe_subset.evaluate(\n",
    "        test_acts_subset,\n",
    "        test_labels_aligned,\n",
    "        threshold=0.388, # we use the optimal threshold obtained above\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        'top_k': top_k,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1'],\n",
    "        'auc': metrics['auc']\n",
    "    })\n",
    "\n",
    "    print(f\"  Results: Acc={metrics['accuracy']:.4f}, Recall={metrics['recall']:.4f}, \"\n",
    "          f\"F1={metrics['f1']:.4f}, AUC={metrics['auc']:.4f}\")\n",
    "\n",
    "    # Get probabilities for hallucination class\n",
    "    y_proba = probe_subset.predict_proba(test_acts_subset)[:, 1]\n",
    "    \n",
    "    # Get precision-recall curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(test_labels_aligned, y_proba)\n",
    "    \n",
    "    # Compute F1 for each threshold\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_idx = f1_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n",
    "    \n",
    "    # Full report with optimized threshold\n",
    "    y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "    print(\"\\nClassification Report (optimized threshold):\")\n",
    "    print(classification_report(test_labels_aligned, y_pred_optimized, digits=4))\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*63)\n",
    "print(\"FEATURE SELECTION RESULTS COMPARISON\")\n",
    "print(\"=\"*63)\n",
    "\n",
    "print(\"\\n{:<12} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"Features\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"\n",
    "))\n",
    "print(\"-\" * 63)\n",
    "\n",
    "for r in results:\n",
    "    print(\"{:<12} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        r['top_k'],\n",
    "        r['accuracy'],\n",
    "        r['precision'],\n",
    "        r['recall'],\n",
    "        r['f1'],\n",
    "        r['auc']\n",
    "    ))\n",
    "\n",
    "# Find best\n",
    "best_f1 = max(results, key=lambda x: x['f1'])\n",
    "best_auc = max(results, key=lambda x: x['auc'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*63)\n",
    "print(\"BEST MODELS\")\n",
    "print(\"=\"*63)\n",
    "\n",
    "print(f\"\\nBest F1: {best_f1['top_k']} features\")\n",
    "print(f\"  Accuracy: {best_f1['accuracy']:.4f}\")\n",
    "print(f\"  Recall:   {best_f1['recall']:.4f}\")\n",
    "print(f\"  F1:       {best_f1['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_f1['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest AUC: {best_auc['top_k']} features\")\n",
    "print(f\"  Accuracy: {best_auc['accuracy']:.4f}\")\n",
    "print(f\"  Recall:   {best_auc['recall']:.4f}\")\n",
    "print(f\"  F1:       {best_auc['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_auc['auc']:.4f}\")\n",
    "\n",
    "# Analyze layer contribution\n",
    "print(\"\\n\" + \"=\"*63)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*63)\n",
    "\n",
    "# First 4096 features correspond to layer 15, next 4096 to layer 16\n",
    "features_15 = [idx for idx in top_indices[:best_f1['top_k']] if idx < 4096]\n",
    "features_16 = [idx for idx in top_indices[:best_f1['top_k']] if idx >= 4096]\n",
    "\n",
    "print(f\"\\nIn top {best_f1['top_k']} features:\")\n",
    "print(f\"  Layer 15 features: {len(features_15)} ({len(features_15)/best_f1['top_k']*100:.1f}%)\")\n",
    "print(f\"  Layer 16 features:  {len(features_16)} ({len(features_16)/best_f1['top_k']*100:.1f}%)\")\n",
    "\n",
    "if len(features_15) > len(features_16) * 1.5:\n",
    "    print(\"\\nLayer 15 contributes more important features.\")\n",
    "elif len(features_16) > len(features_15) * 1.5:\n",
    "    print(\"\\nLayer 16 contributes more important features.\")\n",
    "else:\n",
    "    print(\"\\nBoth layers contribute roughly equally.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*63)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3c34ec-8cdb-4407-aa8c-142e322394da",
   "metadata": {},
   "source": [
    "As anticipated, feature selection does not substantially improve performance metrics. However, the results indicate that comparable performance can be achieved with a reduced feature set, suggesting that the concatenated representation contains considerable redundancy.\n",
    "\n",
    "### 7. One more layer: extending to three layers\n",
    "\n",
    "For completeness, we extend the analysis to include a third layer. We consider the three most expressive layers as established by our earlier experiments: layers 15, 16, and 18.\n",
    "\n",
    "To manage memory efficiently, we employ a streaming concatenation strategy rather than loading all activations simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd3b24b-9b96-4a4c-8ff4-e5beaaceb3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== TRAIN ===\n",
      "Computing total feature dimension...\n",
      "  /workspace/feature_cache15/train_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache16/train_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache18/train_activations_pooled.pt: 4096 features\n",
      "Total features: 12288\n",
      "Creating memmap at /workspace/concat_features/train.dat with shape (101618, 12288)...\n",
      "Loading and copying /workspace/feature_cache15/train_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 0 to 4096)\n",
      "Loading and copying /workspace/feature_cache16/train_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 4096 to 8192)\n",
      "Loading and copying /workspace/feature_cache18/train_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 8192 to 12288)\n",
      "Done.\n",
      "\n",
      "=== VAL ===\n",
      "Computing total feature dimension...\n",
      "  /workspace/feature_cache15/val_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache16/val_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache18/val_activations_pooled.pt: 4096 features\n",
      "Total features: 12288\n",
      "Creating memmap at /workspace/concat_features/val.dat with shape (21775, 12288)...\n",
      "Loading and copying /workspace/feature_cache15/val_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 0 to 4096)\n",
      "Loading and copying /workspace/feature_cache16/val_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 4096 to 8192)\n",
      "Loading and copying /workspace/feature_cache18/val_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 8192 to 12288)\n",
      "Done.\n",
      "\n",
      "=== TEST ===\n",
      "Computing total feature dimension...\n",
      "  /workspace/feature_cache15/test_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache16/test_activations_pooled.pt: 4096 features\n",
      "  /workspace/feature_cache18/test_activations_pooled.pt: 4096 features\n",
      "Total features: 12288\n",
      "Creating memmap at /workspace/concat_features/test.dat with shape (21776, 12288)...\n",
      "Loading and copying /workspace/feature_cache15/test_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 0 to 4096)\n",
      "Loading and copying /workspace/feature_cache16/test_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 4096 to 8192)\n",
      "Loading and copying /workspace/feature_cache18/test_activations_pooled.pt...\n",
      "  Copied 4096 features (columns 8192 to 12288)\n",
      "Done.\n",
      "\n",
      "Final shapes:\n",
      "  Train: (101618, 12288)\n",
      "  Val:   (21775, 12288)\n",
      "  Test:  (21776, 12288)\n",
      "\n",
      "Labels aligned:\n",
      "  Train: 101618\n",
      "  Val:   21775\n",
      "  Test:  21776\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def load_torch_as_numpy(path):\n",
    "    \"\"\"Load a .pt file and convert to numpy float32.\"\"\"\n",
    "    tensor = torch.load(path, map_location=\"cpu\", weights_only=True)\n",
    "    arr = tensor.float().numpy()\n",
    "    del tensor\n",
    "    return arr\n",
    "\n",
    "def build_concat_streaming(output_path, sources, n_samples):\n",
    "    \"\"\"\n",
    "    Build concatenated features by streaming arrays one at a time.\n",
    "    \n",
    "    Args:\n",
    "        output_path: Where to save the memory-mapped output\n",
    "        sources: List of (path, loader_fn) tuples\n",
    "        n_samples: Number of samples to include\n",
    "    \n",
    "    Returns:\n",
    "        np.memmap array of shape (n_samples, total_features)\n",
    "    \"\"\"\n",
    "    # First pass: compute total feature dimension\n",
    "    print(\"Computing total feature dimension...\")\n",
    "    total_dim = 0\n",
    "    dims = []\n",
    "    for path, loader_fn in sources:\n",
    "        arr = loader_fn(path)\n",
    "        dims.append(arr.shape[1])\n",
    "        total_dim += arr.shape[1]\n",
    "        print(f\"  {path}: {arr.shape[1]} features\")\n",
    "        del arr\n",
    "        gc.collect()\n",
    "    \n",
    "    print(f\"Total features: {total_dim}\")\n",
    "    \n",
    "    # Allocate memory-mapped output\n",
    "    print(f\"Creating memmap at {output_path} with shape ({n_samples}, {total_dim})...\")\n",
    "    out = np.memmap(output_path, dtype=np.float32, mode='w+', shape=(n_samples, total_dim))\n",
    "    \n",
    "    # Second pass: fill the array\n",
    "    col = 0\n",
    "    for i, (path, loader_fn) in enumerate(sources):\n",
    "        print(f\"Loading and copying {path}...\")\n",
    "        arr = loader_fn(path).astype(np.float32, copy=False)\n",
    "        width = arr.shape[1]\n",
    "        out[:, col:col + width] = arr[:n_samples]\n",
    "        out.flush()  # Ensure data is written to disk\n",
    "        col += width\n",
    "        print(f\"  Copied {width} features (columns {col - width} to {col})\")\n",
    "        del arr\n",
    "        gc.collect()\n",
    "    \n",
    "    print(\"Done.\")\n",
    "    return out\n",
    "\n",
    "\n",
    "# Define sources for each split\n",
    "train_sources = [\n",
    "    (\"/workspace/feature_cache15/train_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache16/train_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache18/train_activations_pooled.pt\", load_torch_as_numpy),\n",
    "]\n",
    "\n",
    "val_sources = [\n",
    "    (\"/workspace/feature_cache15/val_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache16/val_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache18/val_activations_pooled.pt\", load_torch_as_numpy),\n",
    "]\n",
    "\n",
    "test_sources = [\n",
    "    (\"/workspace/feature_cache15/test_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache16/test_activations_pooled.pt\", load_torch_as_numpy),\n",
    "    (\"/workspace/feature_cache18/test_activations_pooled.pt\", load_torch_as_numpy),\n",
    "]\n",
    "\n",
    "# Sample counts\n",
    "n_train = 101618\n",
    "n_val = 21775\n",
    "n_test = 21776\n",
    "\n",
    "# Output directory for memmaps\n",
    "output_dir = Path(\"/workspace/concat_features\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Build concatenated features\n",
    "print(\"\\n=== TRAIN ===\")\n",
    "train_acts = build_concat_streaming(output_dir / \"train.dat\", train_sources, n_train)\n",
    "\n",
    "print(\"\\n=== VAL ===\")\n",
    "val_acts = build_concat_streaming(output_dir / \"val.dat\", val_sources, n_val)\n",
    "\n",
    "print(\"\\n=== TEST ===\")\n",
    "test_acts = build_concat_streaming(output_dir / \"test.dat\", test_sources, n_test)\n",
    "\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  Train: {train_acts.shape}\")\n",
    "print(f\"  Val:   {val_acts.shape}\")\n",
    "print(f\"  Test:  {test_acts.shape}\")\n",
    "\n",
    "# Labels\n",
    "train_labels_aligned = train_labels[:n_train]\n",
    "val_labels_aligned = val_labels[:n_val]\n",
    "test_labels_aligned = test_labels[:n_test]\n",
    "\n",
    "print(f\"\\nLabels aligned:\")\n",
    "print(f\"  Train: {len(train_labels_aligned)}\")\n",
    "print(f\"  Val:   {len(val_labels_aligned)}\")\n",
    "print(f\"  Test:  {len(test_labels_aligned)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2308b682-580d-42e8-b36d-5cc9335983ea",
   "metadata": {},
   "source": [
    "With the three-layer concatenation prepared, we train the probe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3edc1b9-5af3-4bb7-b7a5-af4b30296fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [13:06:25] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.61336\tval-logloss:0.61359\n",
      "[10]\ttrain-logloss:0.51569\tval-logloss:0.51716\n",
      "[20]\ttrain-logloss:0.46462\tval-logloss:0.46747\n",
      "[30]\ttrain-logloss:0.43453\tval-logloss:0.43888\n",
      "[40]\ttrain-logloss:0.41559\tval-logloss:0.42110\n",
      "[50]\ttrain-logloss:0.40103\tval-logloss:0.40832\n",
      "[60]\ttrain-logloss:0.38867\tval-logloss:0.39830\n",
      "[70]\ttrain-logloss:0.37703\tval-logloss:0.38960\n",
      "[80]\ttrain-logloss:0.36737\tval-logloss:0.38316\n",
      "[90]\ttrain-logloss:0.35892\tval-logloss:0.37790\n",
      "[100]\ttrain-logloss:0.35153\tval-logloss:0.37356\n",
      "[110]\ttrain-logloss:0.34483\tval-logloss:0.37029\n",
      "[120]\ttrain-logloss:0.33892\tval-logloss:0.36739\n",
      "[130]\ttrain-logloss:0.33311\tval-logloss:0.36471\n",
      "[140]\ttrain-logloss:0.32770\tval-logloss:0.36206\n",
      "[150]\ttrain-logloss:0.32282\tval-logloss:0.35997\n",
      "[160]\ttrain-logloss:0.31844\tval-logloss:0.35839\n",
      "[170]\ttrain-logloss:0.31408\tval-logloss:0.35684\n",
      "[180]\ttrain-logloss:0.31008\tval-logloss:0.35553\n",
      "[190]\ttrain-logloss:0.30602\tval-logloss:0.35426\n",
      "[200]\ttrain-logloss:0.30214\tval-logloss:0.35312\n",
      "[210]\ttrain-logloss:0.29820\tval-logloss:0.35217\n",
      "[220]\ttrain-logloss:0.29502\tval-logloss:0.35137\n",
      "[230]\ttrain-logloss:0.29175\tval-logloss:0.35054\n",
      "[240]\ttrain-logloss:0.28814\tval-logloss:0.34964\n",
      "[250]\ttrain-logloss:0.28454\tval-logloss:0.34894\n",
      "[260]\ttrain-logloss:0.28139\tval-logloss:0.34833\n",
      "[270]\ttrain-logloss:0.27856\tval-logloss:0.34767\n",
      "[280]\ttrain-logloss:0.27553\tval-logloss:0.34705\n",
      "[290]\ttrain-logloss:0.27262\tval-logloss:0.34638\n",
      "[300]\ttrain-logloss:0.26983\tval-logloss:0.34591\n",
      "[310]\ttrain-logloss:0.26742\tval-logloss:0.34559\n",
      "[320]\ttrain-logloss:0.26455\tval-logloss:0.34493\n",
      "[330]\ttrain-logloss:0.26201\tval-logloss:0.34439\n",
      "[340]\ttrain-logloss:0.25941\tval-logloss:0.34415\n",
      "[350]\ttrain-logloss:0.25692\tval-logloss:0.34372\n",
      "[360]\ttrain-logloss:0.25445\tval-logloss:0.34333\n",
      "[370]\ttrain-logloss:0.25193\tval-logloss:0.34297\n",
      "[380]\ttrain-logloss:0.24956\tval-logloss:0.34261\n",
      "[390]\ttrain-logloss:0.24768\tval-logloss:0.34215\n",
      "[400]\ttrain-logloss:0.24523\tval-logloss:0.34185\n",
      "[410]\ttrain-logloss:0.24293\tval-logloss:0.34170\n",
      "[420]\ttrain-logloss:0.24067\tval-logloss:0.34123\n",
      "[430]\ttrain-logloss:0.23875\tval-logloss:0.34091\n",
      "[440]\ttrain-logloss:0.23649\tval-logloss:0.34071\n",
      "[450]\ttrain-logloss:0.23419\tval-logloss:0.34055\n",
      "[460]\ttrain-logloss:0.23214\tval-logloss:0.34037\n",
      "[470]\ttrain-logloss:0.23028\tval-logloss:0.34009\n",
      "[480]\ttrain-logloss:0.22812\tval-logloss:0.33982\n",
      "[490]\ttrain-logloss:0.22592\tval-logloss:0.33954\n",
      "[500]\ttrain-logloss:0.22403\tval-logloss:0.33940\n",
      "[510]\ttrain-logloss:0.22204\tval-logloss:0.33922\n",
      "[520]\ttrain-logloss:0.22017\tval-logloss:0.33902\n",
      "[530]\ttrain-logloss:0.21846\tval-logloss:0.33901\n",
      "[540]\ttrain-logloss:0.21635\tval-logloss:0.33881\n",
      "[550]\ttrain-logloss:0.21456\tval-logloss:0.33871\n",
      "[560]\ttrain-logloss:0.21281\tval-logloss:0.33858\n",
      "[570]\ttrain-logloss:0.21098\tval-logloss:0.33846\n",
      "[580]\ttrain-logloss:0.20909\tval-logloss:0.33825\n",
      "[590]\ttrain-logloss:0.20730\tval-logloss:0.33816\n",
      "[600]\ttrain-logloss:0.20560\tval-logloss:0.33813\n",
      "[610]\ttrain-logloss:0.20372\tval-logloss:0.33800\n",
      "[620]\ttrain-logloss:0.20183\tval-logloss:0.33794\n",
      "[630]\ttrain-logloss:0.20022\tval-logloss:0.33792\n",
      "[640]\ttrain-logloss:0.19862\tval-logloss:0.33774\n",
      "[650]\ttrain-logloss:0.19687\tval-logloss:0.33777\n",
      "[660]\ttrain-logloss:0.19537\tval-logloss:0.33762\n",
      "[670]\ttrain-logloss:0.19374\tval-logloss:0.33755\n",
      "[680]\ttrain-logloss:0.19174\tval-logloss:0.33746\n",
      "[690]\ttrain-logloss:0.19030\tval-logloss:0.33735\n",
      "[700]\ttrain-logloss:0.18873\tval-logloss:0.33730\n",
      "[710]\ttrain-logloss:0.18709\tval-logloss:0.33734\n",
      "[720]\ttrain-logloss:0.18553\tval-logloss:0.33715\n",
      "[730]\ttrain-logloss:0.18407\tval-logloss:0.33706\n",
      "[740]\ttrain-logloss:0.18262\tval-logloss:0.33701\n",
      "[750]\ttrain-logloss:0.18110\tval-logloss:0.33686\n",
      "[760]\ttrain-logloss:0.17963\tval-logloss:0.33672\n",
      "[770]\ttrain-logloss:0.17836\tval-logloss:0.33670\n",
      "[780]\ttrain-logloss:0.17700\tval-logloss:0.33674\n",
      "[787]\ttrain-logloss:0.17594\tval-logloss:0.33674\n",
      "\n",
      "Training complete. Best iteration: 767\n",
      "Best validation logloss: 0.3367\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "Accuracy:  0.8431\n",
      "Precision: 0.8203\n",
      "Recall:    0.6561\n",
      "F1 Score:  0.7290\n",
      "AUC:       0.9129\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8510    0.9318    0.8896     14769\n",
      "           1     0.8203    0.6561    0.7290      7007\n",
      "\n",
      "    accuracy                         0.8431     21776\n",
      "   macro avg     0.8356    0.7939    0.8093     21776\n",
      "weighted avg     0.8411    0.8431    0.8379     21776\n",
      "\n",
      "============================================================\n",
      "Model saved to hallucination_probe_layers_15_16_18.pkl\n",
      "\n",
      "Probe saved!\n",
      "Best threshold: 0.362\n",
      "Precision: 0.733, Recall: 0.763, F1: 0.747\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8852    0.8681    0.8766     14769\n",
      "           1     0.7329    0.7627    0.7475      7007\n",
      "\n",
      "    accuracy                         0.8342     21776\n",
      "   macro avg     0.8090    0.8154    0.8120     21776\n",
      "weighted avg     0.8362    0.8342    0.8350     21776\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmscan import XGBoostProbe\n",
    "from sklearn.metrics import precision_recall_curve, classification_report\n",
    "\n",
    "# XGBoost parameters\n",
    "XGB_PARAMS = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.05,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'device': 'cuda',\n",
    "    'eval_metric': 'logloss',\n",
    "}\n",
    "\n",
    "# Train\n",
    "print(\"\\nTraining XGBoost...\")\n",
    "probe_three_layers = XGBoostProbe(xgb_params=XGB_PARAMS)\n",
    "probe_three_layers.fit(\n",
    "    train_acts,\n",
    "    train_labels_aligned,\n",
    "    X_val=val_acts,\n",
    "    y_val=val_labels_aligned,\n",
    "    early_stopping_rounds=20,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "metrics = probe_three_layers.evaluate(test_acts, test_labels_aligned, verbose=True)\n",
    "\n",
    "# Save probe\n",
    "probe_three_layers.save(\"hallucination_probe_layers_15_16_18.pkl\")\n",
    "print(\"\\nProbe saved!\")\n",
    "\n",
    "# Get probabilities for hallucination class\n",
    "y_proba = probe_three_layers.predict_proba(test_acts)[:, 1]\n",
    "\n",
    "# Get precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(test_labels_aligned, y_proba)\n",
    "\n",
    "# Compute F1 for each threshold\n",
    "f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "\n",
    "# Find optimal threshold\n",
    "best_idx = f1_scores.argmax()\n",
    "best_threshold = thresholds[best_idx]\n",
    "\n",
    "print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "print(f\"Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n",
    "\n",
    "# Full report with optimized threshold\n",
    "y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "print(\"\\nClassification Report (optimized threshold):\")\n",
    "print(classification_report(test_labels_aligned, y_pred_optimized, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec3a5a5-f026-4772-bde7-8a876e7179f7",
   "metadata": {},
   "source": [
    "We again explore feature selection on the three-layer concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bfa60ba-618e-4b8b-b9f0-5b237348dafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==============================================================\n",
      "FEATURE SELECTION EXPERIMENT\n",
      "==============================================================\n",
      "\n",
      "Extracting feature importance...\n",
      "\tTotal features: 11313\n",
      "\tFeatures with non-zero importance: 11313\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 500 features (indices: [5125, 11078, 8326, 3551, 7262]...[10158, 10757, 6537, 2932, 9600])\n",
      "Subset shapes: (101618, 500), (21775, 500), (21776, 500)\n",
      "Training XGBoost on 500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:37:46] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 595)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8250, Recall=0.7628, F1=0.7372, AUC=0.9071\n",
      "Best threshold: 0.377\n",
      "Precision: 0.731, Recall: 0.749, F1: 0.740\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8796    0.8693    0.8744     14769\n",
      "           1     0.7311    0.7491    0.7400      7007\n",
      "\n",
      "    accuracy                         0.8306     21776\n",
      "   macro avg     0.8053    0.8092    0.8072     21776\n",
      "weighted avg     0.8318    0.8306    0.8311     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 1000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 1000 features (indices: [5125, 11078, 8326, 3551, 7262]...[11275, 3509, 2419, 6403, 2221])\n",
      "Subset shapes: (101618, 1000), (21775, 1000), (21776, 1000)\n",
      "Training XGBoost on 1000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:38:28] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 636)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8271, Recall=0.7602, F1=0.7389, AUC=0.9097\n",
      "Best threshold: 0.346\n",
      "Precision: 0.707, Recall: 0.779, F1: 0.741\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8898    0.8465    0.8676     14769\n",
      "           1     0.7065    0.7789    0.7410      7007\n",
      "\n",
      "    accuracy                         0.8248     21776\n",
      "   macro avg     0.7981    0.8127    0.8043     21776\n",
      "weighted avg     0.8308    0.8248    0.8268     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 1500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 1500 features (indices: [5125, 11078, 8326, 3551, 7262]...[5712, 11186, 9621, 11961, 8790])\n",
      "Subset shapes: (101618, 1500), (21775, 1500), (21776, 1500)\n",
      "Training XGBoost on 1500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:39:47] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 487)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8286, Recall=0.7645, F1=0.7416, AUC=0.9105\n",
      "Best threshold: 0.374\n",
      "Precision: 0.735, Recall: 0.753, F1: 0.744\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8812    0.8711    0.8761     14769\n",
      "           1     0.7347    0.7525    0.7435      7007\n",
      "\n",
      "    accuracy                         0.8329     21776\n",
      "   macro avg     0.8080    0.8118    0.8098     21776\n",
      "weighted avg     0.8341    0.8329    0.8335     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 2000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 2000 features (indices: [5125, 11078, 8326, 3551, 7262]...[11355, 7256, 1089, 6409, 912])\n",
      "Subset shapes: (101618, 2000), (21775, 2000), (21776, 2000)\n",
      "Training XGBoost on 2000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:41:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 581)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8296, Recall=0.7679, F1=0.7436, AUC=0.9113\n",
      "Best threshold: 0.389\n",
      "Precision: 0.747, Recall: 0.745, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8793    0.8800    0.8797     14769\n",
      "           1     0.7467    0.7454    0.7460      7007\n",
      "\n",
      "    accuracy                         0.8367     21776\n",
      "   macro avg     0.8130    0.8127    0.8128     21776\n",
      "weighted avg     0.8366    0.8367    0.8367     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 2500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 2500 features (indices: [5125, 11078, 8326, 3551, 7262]...[6633, 8047, 1064, 8912, 5452])\n",
      "Subset shapes: (101618, 2500), (21775, 2500), (21776, 2500)\n",
      "Training XGBoost on 2500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:43:29] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 692)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8307, Recall=0.7695, F1=0.7452, AUC=0.9121\n",
      "Best threshold: 0.363\n",
      "Precision: 0.724, Recall: 0.769, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8872    0.8609    0.8739     14769\n",
      "           1     0.7241    0.7692    0.7460      7007\n",
      "\n",
      "    accuracy                         0.8314     21776\n",
      "   macro avg     0.8056    0.8151    0.8099     21776\n",
      "weighted avg     0.8347    0.8314    0.8327     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 3000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 3000 features (indices: [5125, 11078, 8326, 3551, 7262]...[275, 10875, 1879, 4583, 7948])\n",
      "Subset shapes: (101618, 3000), (21775, 3000), (21776, 3000)\n",
      "Training XGBoost on 3000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:46:51] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 713)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8307, Recall=0.7679, F1=0.7448, AUC=0.9115\n",
      "Best threshold: 0.374\n",
      "Precision: 0.735, Recall: 0.757, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8830    0.8703    0.8766     14769\n",
      "           1     0.7346    0.7570    0.7456      7007\n",
      "\n",
      "    accuracy                         0.8338     21776\n",
      "   macro avg     0.8088    0.8136    0.8111     21776\n",
      "weighted avg     0.8353    0.8338    0.8344     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 3500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 3500 features (indices: [5125, 11078, 8326, 3551, 7262]...[2343, 5645, 5496, 2444, 5225])\n",
      "Subset shapes: (101618, 3500), (21775, 3500), (21776, 3500)\n",
      "Training XGBoost on 3500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:50:18] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 873)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8318, Recall=0.7610, F1=0.7444, AUC=0.9126\n",
      "Best threshold: 0.382\n",
      "Precision: 0.747, Recall: 0.745, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8790    0.8804    0.8797     14769\n",
      "           1     0.7470    0.7447    0.7459      7007\n",
      "\n",
      "    accuracy                         0.8367     21776\n",
      "   macro avg     0.8130    0.8125    0.8128     21776\n",
      "weighted avg     0.8366    0.8367    0.8366     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 4000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 4000 features (indices: [5125, 11078, 8326, 3551, 7262]...[10800, 2850, 2503, 5926, 7115])\n",
      "Subset shapes: (101618, 4000), (21775, 4000), (21776, 4000)\n",
      "Training XGBoost on 4000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [10:55:22] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 706)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8316, Recall=0.7664, F1=0.7455, AUC=0.9120\n",
      "Best threshold: 0.365\n",
      "Precision: 0.729, Recall: 0.765, F1: 0.746\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8858    0.8648    0.8752     14769\n",
      "           1     0.7286    0.7649    0.7463      7007\n",
      "\n",
      "    accuracy                         0.8327     21776\n",
      "   macro avg     0.8072    0.8149    0.8107     21776\n",
      "weighted avg     0.8352    0.8327    0.8337     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 4500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 4500 features (indices: [5125, 11078, 8326, 3551, 7262]...[5979, 3145, 4464, 1745, 3659])\n",
      "Subset shapes: (101618, 4500), (21775, 4500), (21776, 4500)\n",
      "Training XGBoost on 4500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [11:00:01] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 600)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8307, Recall=0.7651, F1=0.7441, AUC=0.9125\n",
      "Best threshold: 0.339\n",
      "Precision: 0.705, Recall: 0.793, F1: 0.747\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8957    0.8427    0.8684     14769\n",
      "           1     0.7052    0.7932    0.7466      7007\n",
      "\n",
      "    accuracy                         0.8268     21776\n",
      "   macro avg     0.8005    0.8180    0.8075     21776\n",
      "weighted avg     0.8344    0.8268    0.8292     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 5000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 5000 features (indices: [5125, 11078, 8326, 3551, 7262]...[881, 1117, 1496, 10138, 1482])\n",
      "Subset shapes: (101618, 5000), (21775, 5000), (21776, 5000)\n",
      "Training XGBoost on 5000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [11:04:59] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 816)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8311, Recall=0.7649, F1=0.7446, AUC=0.9124\n",
      "Best threshold: 0.383\n",
      "Precision: 0.746, Recall: 0.749, F1: 0.747\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8808    0.8788    0.8798     14769\n",
      "           1     0.7457    0.7493    0.7475      7007\n",
      "\n",
      "    accuracy                         0.8371     21776\n",
      "   macro avg     0.8133    0.8140    0.8136     21776\n",
      "weighted avg     0.8373    0.8371    0.8372     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 5500 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 5500 features (indices: [5125, 11078, 8326, 3551, 7262]...[1755, 154, 5597, 4161, 7798])\n",
      "Subset shapes: (101618, 5500), (21775, 5500), (21776, 5500)\n",
      "Training XGBoost on 5500 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [11:11:42] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 643)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8291, Recall=0.7671, F1=0.7429, AUC=0.9115\n",
      "Best threshold: 0.380\n",
      "Precision: 0.739, Recall: 0.748, F1: 0.744\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8798    0.8749    0.8773     14769\n",
      "           1     0.7394    0.7480    0.7437      7007\n",
      "\n",
      "    accuracy                         0.8341     21776\n",
      "   macro avg     0.8096    0.8115    0.8105     21776\n",
      "weighted avg     0.8346    0.8341    0.8343     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "EXPERIMENT: Top 6000 features\n",
      "==============================================================\n",
      "\n",
      "Selected top 6000 features (indices: [5125, 11078, 8326, 3551, 7262]...[6664, 7906, 616, 7258, 4943])\n",
      "Subset shapes: (101618, 6000), (21775, 6000), (21776, 6000)\n",
      "Training XGBoost on 6000 selected features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/xgboost/callback.py:386: UserWarning: [11:18:21] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"n_estimators\" } are not used.\n",
      "\n",
      "  self.starting_round = model.num_boosted_rounds()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTraining complete (best iteration: 681)\n",
      "Evaluating...\n",
      "  Results: Acc=0.8320, Recall=0.7674, F1=0.7461, AUC=0.9122\n",
      "Best threshold: 0.375\n",
      "Precision: 0.740, Recall: 0.756, F1: 0.748\n",
      "\n",
      "Classification Report (optimized threshold):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8831    0.8737    0.8784     14769\n",
      "           1     0.7396    0.7562    0.7478      7007\n",
      "\n",
      "    accuracy                         0.8359     21776\n",
      "   macro avg     0.8113    0.8149    0.8131     21776\n",
      "weighted avg     0.8369    0.8359    0.8363     21776\n",
      "\n",
      "\n",
      "==============================================================\n",
      "FEATURE SELECTION RESULTS COMPARISON\n",
      "==============================================================\n",
      "\n",
      "Features     Accuracy   Precision  Recall     F1         AUC       \n",
      "--------------------------------------------------------------\n",
      "500          0.8250     0.7132     0.7628     0.7372     0.9071    \n",
      "1000         0.8271     0.7188     0.7602     0.7389     0.9097    \n",
      "1500         0.8286     0.7200     0.7645     0.7416     0.9105    \n",
      "2000         0.8296     0.7207     0.7679     0.7436     0.9113    \n",
      "2500         0.8307     0.7224     0.7695     0.7452     0.9121    \n",
      "3000         0.8307     0.7231     0.7679     0.7448     0.9115    \n",
      "3500         0.8318     0.7285     0.7610     0.7444     0.9126    \n",
      "4000         0.8316     0.7257     0.7664     0.7455     0.9120    \n",
      "4500         0.8307     0.7243     0.7651     0.7441     0.9125    \n",
      "5000         0.8311     0.7253     0.7649     0.7446     0.9124    \n",
      "5500         0.8291     0.7201     0.7671     0.7429     0.9115    \n",
      "6000         0.8320     0.7260     0.7674     0.7461     0.9122    \n",
      "\n",
      "==============================================================\n",
      "BEST MODELS\n",
      "==============================================================\n",
      "\n",
      "Best F1: 6000 features\n",
      "  Accuracy: 0.8320\n",
      "  Recall:   0.7674\n",
      "  F1:       0.7461\n",
      "  AUC:      0.9122\n",
      "\n",
      "Best AUC: 3500 features\n",
      "  Accuracy: 0.8318\n",
      "  Recall:   0.7610\n",
      "  F1:       0.7444\n",
      "  AUC:      0.9126\n",
      "\n",
      "==============================================================\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "==============================================================\n",
      "\n",
      "In top 6000 features:\n",
      "\tActivation layer 15 features: 2011 (33.5%)\n",
      "\tActivation layer 16 features:  2034 (33.9%)\n",
      "\tActivation layer 18 features:  1955 (32.6%)\n",
      "\n",
      "==============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from llmscan import XGBoostProbe\n",
    "\n",
    "print(\"=\"*62)\n",
    "print(\"FEATURE SELECTION EXPERIMENT\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "# Set feature selection parameters (from 500 to 6000 with a 500 iteration step)\n",
    "MIN_FEATURES = 500\n",
    "MAX_FEATURES = 6000\n",
    "ITERATION_STEP = 500\n",
    "\n",
    "# Extract feature importance\n",
    "print(\"\\nExtracting feature importance...\")\n",
    "feature_importance = probe_three_layers.get_feature_importance(\n",
    "    importance_type='gain',\n",
    "    top_k=None\n",
    ")\n",
    "print(f\"\\tTotal features: {len(feature_importance)}\")\n",
    "print(f\"\\tFeatures with non-zero importance: {len([v for v in feature_importance.values() if v > 0])}\")\n",
    "\n",
    "# Initialize list to store metrics\n",
    "results = []\n",
    "\n",
    "# Iterating over various feature numbers\n",
    "for top_k in range(MIN_FEATURES, MAX_FEATURES+1, ITERATION_STEP):\n",
    "    print(\"\\n\" + \"=\"*62)\n",
    "    print(f\"EXPERIMENT: Top {top_k} features\")\n",
    "    print(\"=\"*62)\n",
    "\n",
    "    # Get top-k most important feature indices\n",
    "    top_features = sorted(\n",
    "        feature_importance.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "    top_indices = [idx for idx, _ in top_features]\n",
    "\n",
    "    print(f\"\\nSelected top {top_k} features (indices: {top_indices[:5]}...{top_indices[-5:]})\")\n",
    "\n",
    "    # Subset the data\n",
    "    train_acts_subset = train_acts[:, top_indices]\n",
    "    val_acts_subset = val_acts[:, top_indices]\n",
    "    test_acts_subset = test_acts[:, top_indices]\n",
    "\n",
    "    print(f\"Subset shapes: {train_acts_subset.shape}, {val_acts_subset.shape}, {test_acts_subset.shape}\")\n",
    "\n",
    "    # Train new XGBoost on selected features\n",
    "    print(f\"Training XGBoost on {top_k} selected features...\")\n",
    "\n",
    "    XGB_PARAMS = {\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'tree_method': 'hist',\n",
    "        'eval_metric': 'logloss',\n",
    "    }\n",
    "\n",
    "    probe_subset = XGBoostProbe(xgb_params=XGB_PARAMS)\n",
    "    probe_subset.fit(\n",
    "        train_acts_subset,\n",
    "        train_labels_aligned,\n",
    "        X_val=val_acts_subset,\n",
    "        y_val=val_labels_aligned,\n",
    "        early_stopping_rounds=20,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    print(f\"\\tTraining complete (best iteration: {probe_subset.model.best_iteration})\")\n",
    "\n",
    "    # Evaluate\n",
    "    print(f\"Evaluating...\")\n",
    "    metrics = probe_subset.evaluate(\n",
    "        test_acts_subset,\n",
    "        test_labels_aligned,\n",
    "        threshold=0.362, # CHANGE THIS !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    results.append({\n",
    "        'top_k': top_k,\n",
    "        'accuracy': metrics['accuracy'],\n",
    "        'precision': metrics['precision'],\n",
    "        'recall': metrics['recall'],\n",
    "        'f1': metrics['f1'],\n",
    "        'auc': metrics['auc']\n",
    "    })\n",
    "\n",
    "    print(f\"  Results: Acc={metrics['accuracy']:.4f}, Recall={metrics['recall']:.4f}, \"\n",
    "          f\"F1={metrics['f1']:.4f}, AUC={metrics['auc']:.4f}\")\n",
    "\n",
    "    # Get probabilities for hallucination class\n",
    "    y_proba = probe_subset.predict_proba(test_acts_subset)[:, 1]\n",
    "    \n",
    "    # Get precision-recall curve\n",
    "    precisions, recalls, thresholds = precision_recall_curve(test_labels_aligned, y_proba)\n",
    "    \n",
    "    # Compute F1 for each threshold\n",
    "    f1_scores = 2 * (precisions * recalls) / (precisions + recalls + 1e-8)\n",
    "    \n",
    "    # Find optimal threshold\n",
    "    best_idx = f1_scores.argmax()\n",
    "    best_threshold = thresholds[best_idx]\n",
    "    \n",
    "    print(f\"Best threshold: {best_threshold:.3f}\")\n",
    "    print(f\"Precision: {precisions[best_idx]:.3f}, Recall: {recalls[best_idx]:.3f}, F1: {f1_scores[best_idx]:.3f}\")\n",
    "    \n",
    "    # Full report with optimized threshold\n",
    "    y_pred_optimized = (y_proba >= best_threshold).astype(int)\n",
    "    print(\"\\nClassification Report (optimized threshold):\")\n",
    "    print(classification_report(test_labels_aligned, y_pred_optimized, digits=4))\n",
    "\n",
    "# Compare results\n",
    "print(\"\\n\" + \"=\"*62)\n",
    "print(\"FEATURE SELECTION RESULTS COMPARISON\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "print(\"\\n{:<12} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(\n",
    "    \"Features\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"\n",
    "))\n",
    "print(\"-\" * 62)\n",
    "\n",
    "for r in results:\n",
    "    print(\"{:<12} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f} {:<10.4f}\".format(\n",
    "        r['top_k'],\n",
    "        r['accuracy'],\n",
    "        r['precision'],\n",
    "        r['recall'],\n",
    "        r['f1'],\n",
    "        r['auc']\n",
    "    ))\n",
    "\n",
    "# Find best\n",
    "best_f1 = max(results, key=lambda x: x['f1'])\n",
    "best_auc = max(results, key=lambda x: x['auc'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*62)\n",
    "print(\"BEST MODELS\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "print(f\"\\nBest F1: {best_f1['top_k']} features\")\n",
    "print(f\"  Accuracy: {best_f1['accuracy']:.4f}\")\n",
    "print(f\"  Recall:   {best_f1['recall']:.4f}\")\n",
    "print(f\"  F1:       {best_f1['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_f1['auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nBest AUC: {best_auc['top_k']} features\")\n",
    "print(f\"  Accuracy: {best_auc['accuracy']:.4f}\")\n",
    "print(f\"  Recall:   {best_auc['recall']:.4f}\")\n",
    "print(f\"  F1:       {best_auc['f1']:.4f}\")\n",
    "print(f\"  AUC:      {best_auc['auc']:.4f}\")\n",
    "\n",
    "# Analyze layer contribution\n",
    "print(\"\\n\" + \"=\"*62)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*62)\n",
    "\n",
    "# Features for activation layers 15, 16 and 18\n",
    "features_15 = [idx for idx in top_indices[:best_f1['top_k']] if idx < 4096]\n",
    "features_16 = [idx for idx in top_indices[:best_f1['top_k']] if idx >= 4096 and idx < 8192]\n",
    "features_18 = [idx for idx in top_indices[:best_f1['top_k']] if idx >= 8192]\n",
    "\n",
    "print(f\"\\nIn top {best_f1['top_k']} features:\")\n",
    "print(f\"\\tActivation layer 15 features: {len(features_15)} ({len(features_15)/best_f1['top_k']*100:.1f}%)\")\n",
    "print(f\"\\tActivation layer 16 features:  {len(features_16)} ({len(features_16)/best_f1['top_k']*100:.1f}%)\")\n",
    "print(f\"\\tActivation layer 18 features:  {len(features_18)} ({len(features_18)/best_f1['top_k']*100:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*62)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc11950-5c78-4a45-9254-f8ef4d15afe0",
   "metadata": {},
   "source": [
    "### 8. Conclusion\n",
    "\n",
    "The results confirm that adding layers beyond the single best-performing one yields no substantial improvement. The following table summarises performance across all configurations:\n",
    "\n",
    "| Layers | Features | Threshold | Accuracy | Precision | Recall | F1 | AUC |\n",
    "|:------:|:--------:|:---------:|:--------:|:---------:|:------:|:--:|:---:|\n",
    "| 16 | All | 0.366 | 82.96 | 72.49 | 75.87 | 74.13 | 91.04 |\n",
    "| 15, 16 | All | 0.388 | **83.75** | **74.94** | 74.35 | 74.65 | 90.99 |\n",
    "| 15, 16 | Top 1500 | 0.361 | 83.17 | 72.53 | 76.79 | 74.60 | 91.10 |\n",
    "| 15, 16 | Top 2500 | 0.371 | 83.49 | 73.53 | 76.07 | **74.78** | 91.06 |\n",
    "| 15, 16, 18 | All | 0.362 | 83.42 | 73.29 | 76.27 | 74.75 | **91.29** |\n",
    "| 15, 16, 18 | Top 3500 | 0.382 | 83.67 | 74.70 | 74.47 | 74.59 | 91.26 |\n",
    "| 15, 16, 18 | Top 4500 | 0.339 | 82.68 | 70.52 | **79.32** | 74.66 | 91.25 |\n",
    "| 15, 16, 18 | Top 5000 | 0.383 | 83.71 | 74.57 | 74.93 | 74.75 | 91.24 |\n",
    "| 15, 16, 18 | Top 6000 | 0.375 | 83.59 | 73.96 | 75.62 | **74.78** | 91.22 |\n",
    "\n",
    "Two key observations emerge from these experiments:\n",
    "\n",
    "1. *Marginal returns from layer concatenation:* Performance plateaus regardless of whether we use one, two, or three layers, with all configurations converging to approximately ~74-75% F1 score, ~83-84% accuracy and ~91% AUC.\n",
    "\n",
    "2. *Localised signal:* The hallucination-related signal encoded in the model's activations does not appear to be distributed across layers. Rather, it can be effectively retrieved from a single layer, with additional layers contributing primarily redundant information.\n",
    "\n",
    "One avenue remains unexplored: training a probe on attention-related features, including multi-head attention outputs and per-head statistics. This will be the focus of the final notebook in this series."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
